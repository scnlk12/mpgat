"""
æ¨¡å‹è¯Šæ–­å·¥å…· - æ·±å…¥åˆ†æå½“å‰æ¨¡å‹çš„è¯¯å·®ï¼Œæ‰¾å‡ºä¼˜åŒ–æ–¹å‘

åŠŸèƒ½:
1. è¯†åˆ«æ¨¡å‹çš„ä¸»è¦å¼±ç‚¹ï¼ˆå“ªäº›ç±»å‹çš„æ ·æœ¬é¢„æµ‹ä¸å¥½ï¼‰
2. åˆ†æè¯¯å·®çš„æ ¹æœ¬åŸå› 
3. ç”Ÿæˆé’ˆå¯¹æ€§çš„ä¼˜åŒ–å»ºè®®
4. å¯è§†åŒ–é«˜è¯¯å·®æ ·æœ¬çš„é¢„æµ‹æƒ…å†µ
"""

import argparse
import os
import csv
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
import datetime

import utils
import data_prepare
from model import GMAN
from utils import cal_lape
from utils.metrics import RMSE_MAE_MAPE


class ModelDiagnostics:
    """æ¨¡å‹è¯Šæ–­å™¨"""

    def __init__(self, model, test_loader, scaler, device, output_dir='./model_diagnosis'):
        self.model = model
        self.test_loader = test_loader
        self.scaler = scaler
        self.device = device
        self.output_dir = output_dir

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'sample_details'), exist_ok=True)

        # å­˜å‚¨æ•°æ®
        self.y_true_all = []
        self.y_pred_all = []
        self.errors_all = []
        self.x_all = []  # è¾“å…¥å†å²æ•°æ®

    @torch.no_grad()
    def collect_predictions(self):
        """æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ"""
        print("æ­£åœ¨æ”¶é›†é¢„æµ‹æ•°æ®...")
        self.model.eval()

        for batch in self.test_loader:
            batch.to_tensor(self.device)
            x_batch = batch['x']
            y_batch = batch['y']

            TE = x_batch[:, :, :, 1:]

            # é¢„æµ‹
            out_batch = self.model(x_batch, TE)
            out_batch = self.scaler.inverse_transform(out_batch)
            y_batch = self.scaler.inverse_transform(y_batch[:, :, :, 0])
            x_batch_inverse = self.scaler.inverse_transform(x_batch[:, :, :, 0])

            # è½¬ä¸ºnumpy
            out_batch = out_batch.cpu().numpy()
            y_batch = y_batch.cpu().numpy()
            x_batch_inverse = x_batch_inverse.cpu().numpy()

            # ä¿å­˜
            for i in range(out_batch.shape[0]):
                self.y_true_all.append(y_batch[i])
                self.y_pred_all.append(out_batch[i])
                self.x_all.append(x_batch_inverse[i])

        # è½¬ä¸ºnumpyæ•°ç»„
        self.y_true_all = np.array(self.y_true_all)  # (samples, timesteps, nodes)
        self.y_pred_all = np.array(self.y_pred_all)
        self.x_all = np.array(self.x_all)  # å†å²æ•°æ®
        self.errors_all = np.abs(self.y_pred_all - self.y_true_all)

        print(f"æ”¶é›†å®Œæˆ: {len(self.y_true_all)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®å½¢çŠ¶: {self.y_true_all.shape}")

    def analyze_error_by_flow_magnitude(self):
        """æŒ‰æµé‡å¤§å°åˆ†æè¯¯å·®"""
        print("\n" + "="*80)
        print("æŒ‰æµé‡å¤§å°åˆ†æè¯¯å·®")
        print("="*80)

        # å®šä¹‰æµé‡åŒºé—´
        bins = [0, 5, 10, 20, 30, 50, 100, float('inf')]
        labels = ['0-5', '5-10', '10-20', '20-30', '30-50', '50-100', '100+']

        true_flat = self.y_true_all.flatten()
        pred_flat = self.y_pred_all.flatten()
        error_flat = self.errors_all.flatten()

        results = []
        print(f"\n{'æµé‡åŒºé—´':<12} {'æ ·æœ¬æ•°':<12} {'å æ¯”':<10} {'å¹³å‡MAE':<12} {'å¹³å‡MAPE':<12} {'RMSE':<12}")
        print("-" * 80)

        for i in range(len(bins) - 1):
            mask = (true_flat >= bins[i]) & (true_flat < bins[i+1])
            count = mask.sum()

            if count > 0:
                mae = error_flat[mask].mean()
                mape = (error_flat[mask] / (true_flat[mask] + 1e-5)).mean() * 100
                rmse = np.sqrt(np.mean((pred_flat[mask] - true_flat[mask]) ** 2))
                ratio = count / len(true_flat) * 100

                results.append({
                    'range': labels[i],
                    'count': count,
                    'ratio': ratio,
                    'mae': mae,
                    'mape': mape,
                    'rmse': rmse
                })

                print(f"{labels[i]:<12} {count:<12} {ratio:<10.2f}% {mae:<12.3f} {mape:<12.2f}% {rmse:<12.3f}")

        # å¯è§†åŒ–
        self._plot_error_by_flow(results)

        # è¯Šæ–­ç»“è®º
        print(f"\nğŸ“Š è¯Šæ–­ç»“è®º:")
        sorted_by_mae = sorted(results, key=lambda x: x['mae'], reverse=True)
        worst = sorted_by_mae[0]
        print(f"  âš ï¸  è¯¯å·®æœ€å¤§çš„æµé‡åŒºé—´: {worst['range']} (MAE={worst['mae']:.3f})")
        print(f"     å æ€»æ ·æœ¬çš„ {worst['ratio']:.1f}%")

        if worst['range'] in ['0-5', '5-10']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print(f"  1. ä½æµé‡é¢„æµ‹ä¸å‡†ï¼Œè€ƒè™‘:")
            print(f"     - ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ï¼Œæé«˜ä½æµé‡æ ·æœ¬çš„æƒé‡")
            print(f"     - å¯¹ä½æµé‡æ ·æœ¬è¿›è¡Œè¿‡é‡‡æ ·")
            print(f"     - æ•°æ®é¢„å¤„ç†ï¼šå¯¹æ•°å˜æ¢ log(x+1)")
        elif worst['range'] in ['50-100', '100+']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print(f"  1. é«˜æµé‡é¢„æµ‹ä¸å‡†ï¼Œè€ƒè™‘:")
            print(f"     - ä½¿ç”¨æ›´é²æ£’çš„æŸå¤±å‡½æ•° (Huber Loss)")
            print(f"     - æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å¼‚å¸¸å€¼")
            print(f"     - å¢åŠ é«˜æµé‡æ ·æœ¬çš„æ•°æ®å¢å¼º")

        return results

    def analyze_error_by_temporal_pattern(self):
        """æŒ‰æ—¶é—´æ¨¡å¼åˆ†æè¯¯å·®"""
        print("\n" + "="*80)
        print("æŒ‰æ—¶é—´æ¨¡å¼åˆ†æè¯¯å·®")
        print("="*80)

        num_samples = self.y_true_all.shape[0]
        num_timesteps = self.y_true_all.shape[1]

        # åˆ†ææ¯ä¸ªæ—¶é—´æ­¥çš„è¯¯å·®
        timestep_stats = []
        for t in range(num_timesteps):
            mae_t = self.errors_all[:, t, :].mean()
            rmse_t = np.sqrt(np.mean((self.y_pred_all[:, t, :] - self.y_true_all[:, t, :]) ** 2))

            # åˆ†æé¢„æµ‹åå·®
            pred_mean = self.y_pred_all[:, t, :].mean()
            true_mean = self.y_true_all[:, t, :].mean()
            bias = pred_mean - true_mean
            bias_ratio = bias / (true_mean + 1e-5) * 100

            timestep_stats.append({
                'timestep': t + 1,
                'mae': mae_t,
                'rmse': rmse_t,
                'bias': bias,
                'bias_ratio': bias_ratio,
                'true_mean': true_mean,
                'pred_mean': pred_mean
            })

        # æ˜¾ç¤º
        print(f"\n{'æ—¶é—´æ­¥':<8} {'MAE':<10} {'RMSE':<10} {'åå·®':<12} {'åå·®ç‡':<12}")
        print("-" * 60)
        for stat in timestep_stats:
            print(f"{stat['timestep']:<8} {stat['mae']:<10.3f} {stat['rmse']:<10.3f} "
                  f"{stat['bias']:<12.3f} {stat['bias_ratio']:<12.2f}%")

        # è¯Šæ–­
        print(f"\nğŸ“Š è¯Šæ–­ç»“è®º:")

        # æ£€æŸ¥è¯¯å·®æ˜¯å¦é€’å¢
        first_3 = np.mean([s['mae'] for s in timestep_stats[:3]])
        last_3 = np.mean([s['mae'] for s in timestep_stats[-3:]])
        growth_rate = (last_3 - first_3) / first_3 * 100

        print(f"  å‰3æ­¥å¹³å‡MAE: {first_3:.3f}")
        print(f"  å3æ­¥å¹³å‡MAE: {last_3:.3f}")
        print(f"  è¯¯å·®å¢é•¿ç‡: {growth_rate:+.1f}%")

        if growth_rate > 30:
            print(f"\n  âš ï¸  é•¿æœŸé¢„æµ‹èƒ½åŠ›è¾ƒå¼±!")
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print(f"  1. å¢å¼ºæ—¶é—´å»ºæ¨¡èƒ½åŠ›:")
            print(f"     - å¢åŠ æ—¶é—´æ³¨æ„åŠ›å±‚çš„æ·±åº¦ (Lå‚æ•°)")
            print(f"     - ä½¿ç”¨æ›´å¤§çš„æ—¶é—´æ³¨æ„åŠ›çª—å£")
            print(f"  2. ä½¿ç”¨æ—¶é—´æ­¥åŠ æƒæŸå¤±:")
            print(f"     - å¯¹åé¢çš„æ—¶é—´æ­¥èµ‹äºˆæ›´é«˜æƒé‡")
            print(f"     - å‚è€ƒ utils/metrics.py ä¸­çš„ temporal_weighted_loss")
            print(f"  3. å¢åŠ å†å²çª—å£:")
            print(f"     - å½“å‰P={timestep_stats[0]['timestep']-1}ï¼Œå¯å°è¯•å¢å¤§åˆ°18æˆ–24")

        # æ£€æŸ¥ç³»ç»Ÿæ€§åå·®
        avg_bias_ratio = np.mean([abs(s['bias_ratio']) for s in timestep_stats])
        if avg_bias_ratio > 10:
            bias_direction = "è¿‡é¢„æµ‹" if np.mean([s['bias']) for s in timestep_stats]) > 0 else "æ¬ é¢„æµ‹"
            print(f"\n  âš ï¸  å­˜åœ¨ç³»ç»Ÿæ€§{bias_direction}! (å¹³å‡åå·®ç‡: {avg_bias_ratio:.1f}%)")
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print(f"  1. æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–æ–¹æ³•")
            print(f"  2. å°è¯•ä¸åŒçš„æŸå¤±å‡½æ•°")
            print(f"  3. æ·»åŠ åå·®æ ¡æ­£å±‚")

        # å¯è§†åŒ–
        self._plot_temporal_error_pattern(timestep_stats)

        return timestep_stats

    def analyze_error_by_node_type(self):
        """æŒ‰èŠ‚ç‚¹ç±»å‹åˆ†æè¯¯å·®"""
        print("\n" + "="*80)
        print("æŒ‰èŠ‚ç‚¹ç‰¹å¾åˆ†æè¯¯å·®")
        print("="*80)

        num_nodes = self.y_true_all.shape[2]

        # åˆ†ææ¯ä¸ªèŠ‚ç‚¹
        node_stats = []
        for n in range(num_nodes):
            mae_n = self.errors_all[:, :, n].mean()
            rmse_n = np.sqrt(np.mean((self.y_pred_all[:, :, n] - self.y_true_all[:, :, n]) ** 2))
            true_mean = self.y_true_all[:, :, n].mean()
            true_std = self.y_true_all[:, :, n].std()
            pred_std = self.y_pred_all[:, :, n].std()

            # æµé‡å˜å¼‚ç³»æ•°
            cv = true_std / (true_mean + 1e-5)

            node_stats.append({
                'node': n,
                'mae': mae_n,
                'rmse': rmse_n,
                'true_mean': true_mean,
                'true_std': true_std,
                'pred_std': pred_std,
                'cv': cv
            })

        df_nodes = pd.DataFrame(node_stats)

        # æŒ‰MAEåˆ†ç»„
        df_nodes['error_level'] = pd.cut(df_nodes['mae'],
                                          bins=[0, df_nodes['mae'].quantile(0.5),
                                                df_nodes['mae'].quantile(0.8),
                                                float('inf')],
                                          labels=['Low Error', 'Medium Error', 'High Error'])

        # åˆ†æé«˜è¯¯å·®èŠ‚ç‚¹çš„ç‰¹å¾
        high_error_nodes = df_nodes[df_nodes['error_level'] == 'High Error']

        print(f"\né«˜è¯¯å·®èŠ‚ç‚¹ (Top 20%):")
        print(f"  èŠ‚ç‚¹æ•°: {len(high_error_nodes)}")
        print(f"  å¹³å‡MAE: {high_error_nodes['mae'].mean():.3f}")
        print(f"  å¹³å‡æµé‡: {high_error_nodes['true_mean'].mean():.2f}")
        print(f"  å¹³å‡å˜å¼‚ç³»æ•°: {high_error_nodes['cv'].mean():.3f}")

        # æ˜¾ç¤ºæœ€å·®çš„10ä¸ªèŠ‚ç‚¹
        worst_nodes = df_nodes.nlargest(10, 'mae')
        print(f"\nMAEæœ€é«˜çš„10ä¸ªèŠ‚ç‚¹:")
        print(f"{'èŠ‚ç‚¹':<8} {'MAE':<10} {'å¹³å‡æµé‡':<12} {'æµé‡std':<12} {'å˜å¼‚ç³»æ•°':<12}")
        print("-" * 60)
        for _, row in worst_nodes.iterrows():
            print(f"{int(row['node']):<8} {row['mae']:<10.3f} {row['true_mean']:<12.2f} "
                  f"{row['true_std']:<12.2f} {row['cv']:<12.3f}")

        # è¯Šæ–­
        print(f"\nğŸ“Š è¯Šæ–­ç»“è®º:")

        # æ£€æŸ¥é«˜è¯¯å·®èŠ‚ç‚¹çš„ç‰¹å¾
        high_cv = high_error_nodes['cv'].mean()
        low_error_nodes = df_nodes[df_nodes['error_level'] == 'Low Error']
        low_cv = low_error_nodes['cv'].mean()

        print(f"  é«˜è¯¯å·®èŠ‚ç‚¹å¹³å‡å˜å¼‚ç³»æ•°: {high_cv:.3f}")
        print(f"  ä½è¯¯å·®èŠ‚ç‚¹å¹³å‡å˜å¼‚ç³»æ•°: {low_cv:.3f}")

        if high_cv > low_cv * 1.5:
            print(f"\n  âš ï¸  é«˜è¯¯å·®èŠ‚ç‚¹çš„æµé‡æ³¢åŠ¨æ›´å¤§!")
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print(f"  1. å¢å¼ºå¯¹æ³¢åŠ¨çš„å»ºæ¨¡èƒ½åŠ›:")
            print(f"     - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ")
            print(f"     - å¢åŠ æ³¨æ„åŠ›å¤´æ•°K")
            print(f"  2. å¯¹é«˜æ³¢åŠ¨èŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„ç­–ç•¥:")
            print(f"     - èŠ‚ç‚¹çº§åˆ«çš„æ³¨æ„åŠ›æƒé‡")
            print(f"     - è‡ªé€‚åº”çš„æ­£åˆ™åŒ–")

        # ä¿å­˜èŠ‚ç‚¹åˆ†æç»“æœ
        csv_path = os.path.join(self.output_dir, 'node_analysis.csv')
        df_nodes.to_csv(csv_path, index=False)
        print(f"\n  å·²ä¿å­˜èŠ‚ç‚¹åˆ†æç»“æœ: node_analysis.csv")

        # å¯è§†åŒ–
        self._plot_node_analysis(df_nodes)

        return df_nodes

    def visualize_worst_samples(self, top_k=10):
        """å¯è§†åŒ–æœ€å·®çš„æ ·æœ¬"""
        print("\n" + "="*80)
        print(f"å¯è§†åŒ–æœ€å·®çš„ {top_k} ä¸ªæ ·æœ¬")
        print("="*80)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è¯¯å·®
        sample_mae = self.errors_all.mean(axis=(1, 2))
        worst_indices = np.argsort(sample_mae)[-top_k:][::-1]

        for rank, idx in enumerate(worst_indices, 1):
            mae = sample_mae[idx]
            y_true = self.y_true_all[idx]  # (timesteps, nodes)
            y_pred = self.y_pred_all[idx]
            x_hist = self.x_all[idx]  # å†å²æ•°æ®

            # æ‰¾å‡ºè¯¯å·®æœ€å¤§çš„èŠ‚ç‚¹
            node_mae = self.errors_all[idx].mean(axis=0)  # (nodes,)
            top_error_node = np.argmax(node_mae)

            print(f"\næ ·æœ¬ #{rank} (ç´¢å¼•={idx}, MAE={mae:.3f})")
            print(f"  è¯¯å·®æœ€å¤§çš„èŠ‚ç‚¹: Node {top_error_node} (MAE={node_mae[top_error_node]:.3f})")

            # å¯è§†åŒ–è¯¥èŠ‚ç‚¹çš„é¢„æµ‹
            self._plot_sample_prediction(
                idx, top_error_node, x_hist[:, top_error_node],
                y_true[:, top_error_node], y_pred[:, top_error_node],
                mae, rank
            )

    def _plot_sample_prediction(self, sample_idx, node_idx, x_hist, y_true, y_pred, mae, rank):
        """ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„é¢„æµ‹æ›²çº¿"""
        fig, ax = plt.subplots(figsize=(12, 6))

        hist_len = len(x_hist)
        pred_len = len(y_true)

        # æ—¶é—´è½´
        hist_time = list(range(-hist_len, 0))
        pred_time = list(range(0, pred_len))

        # ç»˜åˆ¶å†å²
        ax.plot(hist_time, x_hist, 'o-', color='gray', label='Historical', linewidth=2, markersize=5)

        # ç»˜åˆ¶çœŸå®å€¼å’Œé¢„æµ‹å€¼
        ax.plot(pred_time, y_true, 'o-', color='green', label='True', linewidth=2, markersize=6)
        ax.plot(pred_time, y_pred, 's--', color='red', label='Predicted', linewidth=2, markersize=6)

        # æ ‡æ³¨è¯¯å·®
        errors = np.abs(y_pred - y_true)
        for t, err in enumerate(errors):
            if err > mae * 1.5:  # æ ‡æ³¨è¶…è¿‡å¹³å‡è¯¯å·®1.5å€çš„ç‚¹
                ax.annotate(f'{err:.1f}',
                           xy=(pred_time[t], y_pred[t]),
                           xytext=(0, 10),
                           textcoords='offset points',
                           ha='center',
                           fontsize=8,
                           color='red')

        ax.axvline(x=0, color='black', linestyle=':', linewidth=1, alpha=0.5)
        ax.set_xlabel('Time Step', fontsize=12)
        ax.set_ylabel('Flow Value', fontsize=12)
        ax.set_title(f'Worst Sample #{rank} (Sample {sample_idx}, Node {node_idx}, MAE={mae:.3f})',
                    fontsize=13, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        filename = f'worst_sample_{rank}_idx{sample_idx}_node{node_idx}.png'
        plt.savefig(os.path.join(self.output_dir, 'sample_details', filename), dpi=300)
        plt.close()

        print(f"    å·²ä¿å­˜: sample_details/{filename}")

    def _plot_error_by_flow(self, results):
        """ç»˜åˆ¶æŒ‰æµé‡åˆ†ç»„çš„è¯¯å·®"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        ranges = [r['range'] for r in results]
        maes = [r['mae'] for r in results]
        ratios = [r['ratio'] for r in results]

        # MAEæŸ±çŠ¶å›¾
        ax1.bar(ranges, maes, alpha=0.7, color='steelblue')
        ax1.set_xlabel('Flow Range', fontsize=12)
        ax1.set_ylabel('MAE', fontsize=12)
        ax1.set_title('MAE by Flow Range', fontsize=13)
        ax1.grid(True, alpha=0.3, axis='y')
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)

        # æ ·æœ¬åˆ†å¸ƒé¥¼å›¾
        ax2.pie(ratios, labels=ranges, autopct='%1.1f%%', startangle=90)
        ax2.set_title('Sample Distribution by Flow Range', fontsize=13)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'plots', 'error_by_flow.png'), dpi=300)
        plt.close()
        print(f"  å·²ä¿å­˜: plots/error_by_flow.png")

    def _plot_temporal_error_pattern(self, timestep_stats):
        """ç»˜åˆ¶æ—¶é—´æ­¥è¯¯å·®æ¨¡å¼"""
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))

        timesteps = [s['timestep'] for s in timestep_stats]
        maes = [s['mae'] for s in timestep_stats]
        biases = [s['bias'] for s in timestep_stats]

        # MAEæ›²çº¿
        axes[0].plot(timesteps, maes, 'o-', linewidth=2, markersize=6, color='steelblue')
        axes[0].set_xlabel('Prediction Timestep', fontsize=12)
        axes[0].set_ylabel('MAE', fontsize=12)
        axes[0].set_title('MAE by Prediction Timestep', fontsize=13)
        axes[0].grid(True, alpha=0.3)

        # åå·®æ›²çº¿
        axes[1].plot(timesteps, biases, 'o-', linewidth=2, markersize=6, color='coral')
        axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)
        axes[1].set_xlabel('Prediction Timestep', fontsize=12)
        axes[1].set_ylabel('Bias (Pred - True)', fontsize=12)
        axes[1].set_title('Prediction Bias by Timestep', fontsize=13)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'plots', 'temporal_error_pattern.png'), dpi=300)
        plt.close()
        print(f"  å·²ä¿å­˜: plots/temporal_error_pattern.png")

    def _plot_node_analysis(self, df_nodes):
        """ç»˜åˆ¶èŠ‚ç‚¹åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # MAEåˆ†å¸ƒ
        axes[0, 0].hist(df_nodes['mae'], bins=50, edgecolor='black', alpha=0.7)
        axes[0, 0].set_xlabel('MAE', fontsize=11)
        axes[0, 0].set_ylabel('Number of Nodes', fontsize=11)
        axes[0, 0].set_title('Distribution of Node MAE', fontsize=12)
        axes[0, 0].grid(True, alpha=0.3)

        # MAE vs å¹³å‡æµé‡
        axes[0, 1].scatter(df_nodes['true_mean'], df_nodes['mae'], alpha=0.5, s=30)
        axes[0, 1].set_xlabel('Average Flow', fontsize=11)
        axes[0, 1].set_ylabel('MAE', fontsize=11)
        axes[0, 1].set_title('MAE vs Average Flow', fontsize=12)
        axes[0, 1].grid(True, alpha=0.3)

        # MAE vs å˜å¼‚ç³»æ•°
        axes[1, 0].scatter(df_nodes['cv'], df_nodes['mae'], alpha=0.5, s=30, color='coral')
        axes[1, 0].set_xlabel('Coefficient of Variation', fontsize=11)
        axes[1, 0].set_ylabel('MAE', fontsize=11)
        axes[1, 0].set_title('MAE vs Flow Variability', fontsize=12)
        axes[1, 0].grid(True, alpha=0.3)

        # Top20é«˜è¯¯å·®èŠ‚ç‚¹
        top20 = df_nodes.nlargest(20, 'mae')
        axes[1, 1].barh(range(len(top20)), top20['mae'].values)
        axes[1, 1].set_yticks(range(len(top20)))
        axes[1, 1].set_yticklabels([f"Node {int(n)}" for n in top20['node'].values], fontsize=8)
        axes[1, 1].set_xlabel('MAE', fontsize=11)
        axes[1, 1].set_title('Top 20 Nodes by MAE', fontsize=12)
        axes[1, 1].invert_yaxis()
        axes[1, 1].grid(True, alpha=0.3, axis='x')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'plots', 'node_analysis.png'), dpi=300)
        plt.close()
        print(f"  å·²ä¿å­˜: plots/node_analysis.png")

    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ç”Ÿæˆä¼˜åŒ–å»ºè®®æŠ¥å‘Š")
        print("="*80)

        report_path = os.path.join(self.output_dir, 'optimization_suggestions.txt')

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("æ¨¡å‹ä¼˜åŒ–å»ºè®®æŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}\n")
            f.write("="*80 + "\n\n")

            f.write("åŸºäºè¯¯å·®åˆ†æï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹æ€§çš„ä¼˜åŒ–å»ºè®®:\n\n")

            f.write("ğŸ“Œ ä¼˜å…ˆçº§1: ç«‹å³å°è¯•\n")
            f.write("-" * 40 + "\n")
            f.write("1. è°ƒæ•´æŸå¤±å‡½æ•°\n")
            f.write("   - å¦‚æœä½æµé‡è¯¯å·®å¤§: ä½¿ç”¨åŠ æƒMAEæˆ–Focal Loss\n")
            f.write("   - å¦‚æœæœ‰å¼‚å¸¸å€¼: ä½¿ç”¨Huber Loss\n")
            f.write("   - å¦‚æœé•¿æœŸé¢„æµ‹å·®: ä½¿ç”¨æ—¶é—´æ­¥åŠ æƒæŸå¤±\n")
            f.write("   å‚è€ƒ: utils/metrics.py ä¸­çš„æŸå¤±å‡½æ•°\n\n")

            f.write("2. æ•°æ®é¢„å¤„ç†\n")
            f.write("   - æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–æ–¹æ³• (StandardScaler vs MinMaxScaler)\n")
            f.write("   - å¯¹ä½æµé‡æ•°æ®: å°è¯• log(x+1) å˜æ¢\n")
            f.write("   - æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å¼‚å¸¸å€¼éœ€è¦å¤„ç†\n\n")

            f.write("ğŸ“Œ ä¼˜å…ˆçº§2: è°ƒæ•´è¶…å‚æ•°\n")
            f.write("-" * 40 + "\n")
            f.write("1. å¦‚æœé•¿æœŸé¢„æµ‹èƒ½åŠ›å¼±:\n")
            f.write("   - å¢åŠ å†å²çª—å£ P (12 -> 18 æˆ– 24)\n")
            f.write("   - å¢åŠ STAttå±‚æ•° L (2 -> 3)\n")
            f.write("   - å¢åŠ æ³¨æ„åŠ›å¤´æ•° K (8 -> 12)\n\n")

            f.write("2. å¦‚æœç‰¹å®šèŠ‚ç‚¹è¯¯å·®å¤§:\n")
            f.write("   - æ£€æŸ¥å›¾ç»“æ„æ˜¯å¦åˆç†\n")
            f.write("   - è°ƒæ•´ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶\n")
            f.write("   - è€ƒè™‘æ·»åŠ èŠ‚ç‚¹ç‰¹å¾\n\n")

            f.write("ğŸ“Œ ä¼˜å…ˆçº§3: è®­ç»ƒç­–ç•¥\n")
            f.write("-" * 40 + "\n")
            f.write("1. å­¦ä¹ ç‡è°ƒæ•´\n")
            f.write("   - é™ä½åˆå§‹å­¦ä¹ ç‡ (0.001 -> 0.0005)\n")
            f.write("   - ä½¿ç”¨warmupç­–ç•¥\n")
            f.write("   - å°è¯•Cosine Annealing\n\n")

            f.write("2. æ­£åˆ™åŒ–\n")
            f.write("   - æ·»åŠ weight decay\n")
            f.write("   - æ·»åŠ Dropout (0.1-0.3)\n")
            f.write("   - Label smoothing\n\n")

            f.write("3. æ•°æ®å¢å¼º\n")
            f.write("   - å¯¹é«˜è¯¯å·®æ ·æœ¬è¿‡é‡‡æ ·\n")
            f.write("   - æ—¶é—´çª—å£æ»‘åŠ¨é‡‡æ ·\n\n")

            f.write("="*80 + "\n")
            f.write("è¯¦ç»†åˆ†æè¯·æŸ¥çœ‹å…¶ä»–è¾“å‡ºæ–‡ä»¶\n")
            f.write("="*80 + "\n")

        print(f"  å·²ä¿å­˜ä¼˜åŒ–å»ºè®®: optimization_suggestions.txt")

    def run_full_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("\n" + "="*80)
        print("å¼€å§‹æ¨¡å‹è¯Šæ–­")
        print("="*80)

        # 1. æ”¶é›†æ•°æ®
        self.collect_predictions()

        # 2. æ•´ä½“ç»Ÿè®¡
        rmse, mae, mape = RMSE_MAE_MAPE(self.y_true_all, self.y_pred_all)
        print(f"\næ•´ä½“æ€§èƒ½:")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  MAE:  {mae:.4f}")
        print(f"  MAPE: {mape:.4f}%")

        # 3. æŒ‰æµé‡å¤§å°åˆ†æ
        flow_results = self.analyze_error_by_flow_magnitude()

        # 4. æŒ‰æ—¶é—´æ¨¡å¼åˆ†æ
        temporal_results = self.analyze_error_by_temporal_pattern()

        # 5. æŒ‰èŠ‚ç‚¹åˆ†æ
        node_results = self.analyze_error_by_node_type()

        # 6. å¯è§†åŒ–æœ€å·®æ ·æœ¬
        self.visualize_worst_samples(top_k=10)

        # 7. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        self.generate_optimization_report()

        print(f"\n" + "="*80)
        print(f"è¯Šæ–­å®Œæˆ! æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print(f"  - plots/                  å¯è§†åŒ–å›¾è¡¨")
        print(f"  - sample_details/         é«˜è¯¯å·®æ ·æœ¬è¯¦æƒ…")
        print(f"  - node_analysis.csv       èŠ‚ç‚¹åˆ†ææ•°æ®")
        print(f"  - optimization_suggestions.txt  ä¼˜åŒ–å»ºè®®")
        print("="*80)


def load_model_and_data(args):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print("åŠ è½½æ•°æ®...")
    _, _, test_loader, scaler = data_prepare.get_dataloaders(args, log=None)

    # æ•°æ®é›†ä¿¡æ¯
    dataset_name = args.traffic_file.split('/')[-1].replace('.npz', '')
    dataset_dir = '/'.join(args.traffic_file.split('/')[:-1])
    csv_file = os.path.join(dataset_dir, f'{dataset_name}.csv')
    txt_file = os.path.join(dataset_dir, f'{dataset_name}.txt')

    # è¯»å–èŠ‚ç‚¹
    temp_nodes = set()
    with open(csv_file, 'r') as f:
        f.readline()
        reader = csv.reader(f)
        for row in reader:
            if len(row) != 3:
                continue
            temp_nodes.add(int(row[0]))
            temp_nodes.add(int(row[1]))

    if os.path.exists(txt_file):
        with open(txt_file, 'r') as f:
            id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\n'))}
        num_nodes = len(id_dict)
    else:
        sorted_nodes = sorted(list(temp_nodes))
        id_dict = {node_id: idx for idx, node_id in enumerate(sorted_nodes)}
        num_nodes = len(sorted_nodes)

    # é‚»æ¥çŸ©é˜µ
    adj_mx = np.zeros((num_nodes, num_nodes), dtype=float)
    with open(csv_file, 'r') as f:
        f.readline()
        reader = csv.reader(f)
        for row in reader:
            if len(row) != 3:
                continue
            i, j, distance = int(row[0]), int(row[1]), float(row[2])
            if i in id_dict and j in id_dict:
                idx_i = id_dict[i]
                idx_j = id_dict[j]
                adj_mx[idx_i][idx_j] = 1
                adj_mx[idx_j][idx_i] = 1

    lap_mx, LAP = cal_lape(adj_mx)
    lap_mx = lap_mx.to(device)

    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    model = GMAN(args.input_dim, args.P, args.Q, args.T, args.L, args.K, args.d, lap_mx, LAP)
    model = model.to(device)

    if args.model_path:
        print(f"ä» {args.model_path} åŠ è½½æ¨¡å‹æƒé‡...")
        model.load_state_dict(torch.load(args.model_path, map_location=device))

    return model, test_loader, scaler, device


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯Šæ–­å·¥å…·')

    parser.add_argument('--time_slot', type=int, default=5)
    parser.add_argument('--P', type=int, default=12)
    parser.add_argument('--Q', type=int, default=12)
    parser.add_argument('--L', type=int, default=2)
    parser.add_argument('--T', type=int, default=288)
    parser.add_argument('--embed_dim', type=int, default=1)
    parser.add_argument('--K', type=int, default=8)
    parser.add_argument('--input_dim', type=int, default=3)
    parser.add_argument('--d', type=int, default=8)
    parser.add_argument('--train_ratio', type=float, default=0.6)
    parser.add_argument('--val_ratio', type=float, default=0.2)
    parser.add_argument('--test_ratio', type=float, default=0.2)
    parser.add_argument('--batch_size', type=int, default=16)

    parser.add_argument('--traffic_file', default='data/PEMS03/PEMS03.npz')
    parser.add_argument('--model_path', required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_dir', default='./model_diagnosis', help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # åŠ è½½å¹¶è¯Šæ–­
    model, test_loader, scaler, device = load_model_and_data(args)
    diagnostics = ModelDiagnostics(model, test_loader, scaler, device, args.output_dir)
    diagnostics.run_full_diagnosis()

    print("\nè¯Šæ–­å®Œæˆ!")